{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Tree Algorithms In Wheat Seeds DATASET\n",
    "\n",
    "##### Using all features\n",
    "| | Decision Tree | Random Forest | Gradient Boosting|\n",
    "| --- | --- | --- |--- |\n",
    "| Training  Accuracy | 100 | 100 | 100 |\n",
    "| Test Accuracy | 92.5 | 92.5 | 92.5 |\n",
    "| Wall Time for finding optimal parameters | 144ms | 32.8s | 23.5s|\n",
    "\n",
    "##### After feature selection\n",
    "| | Decision Tree | Random Forest | Gradient Boosting|\n",
    "| --- | --- | --- |--- |\n",
    "| Training  Accuracy | 98.74 | 98.74 | 100 |\n",
    "| Test Accuracy | 92.5 | 92.5 | 90 |\n",
    "| Wall Time for finding optimal parameters | 66ms | 31.1s | 7.98s|\n",
    "\n",
    "#### Observations\n",
    "- All three methods had lower training accuracy after highly correlated features were removed. \n",
    "- The Decision Tree Classifier had the same performance with the Random Forest Classifier. \n",
    "- Gradient Boosting performed slightly worse on the test set\n",
    "- Feature Selection did not improve the accuracy much, but it reduced the time it took the algorithm to run significantly.\n",
    "- Time taken to run the Decision Tree Classifier regardless of the feature engineering method < Time taken to run the Random Forest Classifier regardless of the feature engineering method < Time taken to run the Gradient Boosting Classifier regardless of the feature engineering method. \n",
    "\n",
    "#### Expectation Vs. Reality\n",
    "\n",
    "- I expected the Decision Tree to have a high training accuracy and this happened\n",
    "- I did not expect the Decision Tree classify to perform the same way as the Random Forest Classifier, I expected the Random Forest Classifier to be better. This may have happened because the dataset is quite small.\n",
    "- Random Forest and Gradient Boosting can be better depending on the situation\n",
    "- I expected Gradient Boosting to outperform Decision Trees but this is not the case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Tree Algorithms In COVID DATASET\n",
    "\n",
    "##### Using all features\n",
    "| | Decision Tree | Random Forest | Gradient Boosting|\n",
    "| --- | --- | --- |--- |\n",
    "| Training  Accuracy | 66.25 | 69.8 | 68.52 |\n",
    "| Test Accuracy | 65.81 | 67.4 | 67.23 |\n",
    "| Wall Time for finding optimal parameters | 1.22s | 88s | 174s|\n",
    "\n",
    "##### Dropping the City column\n",
    "| | Decision Tree | Random Forest | Gradient Boosting|\n",
    "| --- | --- | --- |--- |\n",
    "| Training  Accuracy | 66.14 | 67.08 | 67.19 |\n",
    "| Test Accuracy | 65.91 | 66.15 | 66.52 |\n",
    "| Wall Time for finding optimal parameters | 656ms | 72s | 134s|\n",
    "\n",
    "##### Aggregating the categories of the features\n",
    "| | Decision Tree | Random Forest | Gradient Boosting|\n",
    "| --- | --- | --- |--- |\n",
    "| Training  Accuracy | 66.14 | 66.94 | 68.14 |\n",
    "| Test Accuracy | 65.91 | 67.16 | 67.33 |\n",
    "| Wall Time for finding optimal parameters | 739ms | 79s | 124s |\n",
    "\n",
    "#### Observations\n",
    "- The test accuracy of the Decision Tree Accuracy improved while the test accuracy scores for the other two algorithms experienced fractional reductions\n",
    "- There was a drastic reduction in computation time for all algorithms\n",
    "- Gradient Boost performed best with the aggregated features.\n",
    "- Decision Tree performed better in both models where the city column was excluded or aggregated.\n",
    "- The training and test set accuracy was only slightly better when the city column was included as compared to when the features were aggregated.\n",
    "- Time taken to run the Decision Tree Classifier regardless of the feature engineering method < Time taken to run the Random Forest Classifier regardless of the feature engineering method < Time taken to run the Gradient Boosting Classifier regardless of the feature engineering method.\n",
    "\n",
    "\n",
    "#### Expectation Vs. Reality\n",
    "\n",
    "- I expected Random Forest and Gradient Boosting to outperform Decision trees and that happened\n",
    "- I expected Gradient Boosting and Random Forest to take much longer than Decision trees and that happened.\n",
    "\n",
    "Overall, for the covid dataset, aggregating the features seemed better as it produced comparatively good results and it also saves a lot of computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion from both analysis\n",
    "\n",
    "For simple datasets with little variations and bias, Decision Trees perform excellently well and can rival Random Forest and Gradient Boosting Trees.\n",
    "\n",
    "But for Complex datasets with high variations and bias,ensemble methods seem to perform better than a single Decision Tree.\n",
    "\n",
    "Considering that random forest had to do 20 iterations in finding optimal parameters compared to Gradient Booting Tree that had to do only 5 iteration, it can be concluded the Gradient Boosting Tree is more computationaly expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
